{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "SJJ9XgvPrw3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100 - Loss: 2.2220\n",
            "Epoch 11/100 - Loss: 1.5063\n",
            "Epoch 21/100 - Loss: 1.1915\n",
            "Epoch 31/100 - Loss: 1.0428\n",
            "Epoch 41/100 - Loss: 0.9500\n",
            "Epoch 51/100 - Loss: 0.8854\n",
            "Epoch 61/100 - Loss: 0.8422\n",
            "Epoch 71/100 - Loss: 0.8100\n",
            "Epoch 81/100 - Loss: 0.7861\n",
            "Epoch 91/100 - Loss: 0.7692\n",
            "Epoch 100/100 - Loss: 0.7581\n",
            "\n",
            "✅ Training Accuracy: 84.24%\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load and split the data\n",
        "data = np.load('X_train.npy')\n",
        "labels = np.load('y_train.npy')\n",
        "data = data.reshape(60000,784)\n",
        "X = data/ 255.0  # normalize pixel values\n",
        "y = labels\n",
        "\n",
        "# One-hot encode labels\n",
        "def one_hot_encode(y, num_classes):\n",
        "    m = y.shape[0]\n",
        "    encoded = np.zeros((m, num_classes))\n",
        "    encoded[np.arange(m), y] = 1\n",
        "    return encoded\n",
        "\n",
        "y_encoded = one_hot_encode(y, num_classes=10)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Parameters\n",
        "m, n = X_train.shape\n",
        "num_classes = 10\n",
        "learning_rate = 10\n",
        "epochs = 100\n",
        "batch_size = 64\n",
        "\n",
        "# Initialize weights and bias\n",
        "W = np.random.randn(n, num_classes) * 0.01\n",
        "b = np.zeros((1, num_classes))\n",
        "\n",
        "# Softmax function\n",
        "def softmax(z):\n",
        "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # numerical stability\n",
        "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
        "\n",
        "# Loss function (Cross-Entropy)\n",
        "def compute_loss(y_true, y_pred):\n",
        "    m = y_true.shape[0]\n",
        "    log_likelihood = -np.log(y_pred + 1e-15)\n",
        "    loss = np.sum(y_true * log_likelihood) / m\n",
        "    return loss\n",
        "\n",
        "# Training loop with Mini-Batch Gradient Descent\n",
        "loss_history = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Shuffle training data\n",
        "    indices = np.random.permutation(m)\n",
        "    X_train_shuffled = X_train[indices]\n",
        "    y_train_shuffled = y_train[indices]\n",
        "\n",
        "    for i in range(0, m, batch_size):\n",
        "        X_batch = X_train_shuffled[i:i+batch_size]\n",
        "        y_batch = y_train_shuffled[i:i+batch_size]\n",
        "\n",
        "        # Forward pass\n",
        "        logits = np.dot(X_batch, W) + b\n",
        "        probs = softmax(logits)\n",
        "\n",
        "        # Compute gradients\n",
        "        error = probs - y_batch\n",
        "        dW = np.dot(X_batch.T, error) / batch_size\n",
        "        db = np.sum(error, axis=0, keepdims=True) / batch_size\n",
        "\n",
        "        # Update parameters\n",
        "        W -= learning_rate * dW\n",
        "        b -= learning_rate * db\n",
        "\n",
        "    # Compute loss for tracking\n",
        "    train_logits = np.dot(X_train, W) + b\n",
        "    train_probs = softmax(train_logits)\n",
        "    loss = compute_loss(y_train, train_probs)\n",
        "    loss_history.append(loss)\n",
        "\n",
        "    if epoch % 10 == 0 or epoch == epochs - 1:\n",
        "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss:.4f}\")\n",
        "        # Significantly improved training accuracy by converting to dynamic learning rate\n",
        "        learning_rate = 0.85*learning_rate\n",
        "\n",
        "# Predictions\n",
        "def predict(X):\n",
        "    logits = np.dot(X, W) + b\n",
        "    probs = softmax(logits)\n",
        "    return np.argmax(probs, axis=1)\n",
        "\n",
        "y_test_labels = np.argmax(y_test, axis=1)\n",
        "y_pred = predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test_labels, y_pred)\n",
        "print(f\"\\n✅ Training Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# # Plot loss\n",
        "# plt.plot(loss_history)\n",
        "# plt.xlabel(\"Epoch\")\n",
        "# plt.ylabel(\"Loss\")\n",
        "# plt.title(\"Training Loss over Epochs\")\n",
        "# plt.grid()\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "3lV3WQ_crxos"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'test_images.npy'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[10], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# --- Load the new test data ---\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m test_data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest_images.npy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m y_real_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_train.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(test_data\u001b[38;5;241m.\u001b[39mshape)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\numpy\\lib\\npyio.py:427\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    425\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 427\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    428\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'test_images.npy'"
          ]
        }
      ],
      "source": [
        "# --- Load the new test data ---\n",
        "import pandas as pd\n",
        "test_data = np.load(\"test_images.npy\")\n",
        "y_real_test = np.load(\"y_train.npy\")\n",
        "print(test_data.shape)\n",
        "X_real_test = test_data.iloc[:, 1:].values / 255.0  # normalize\n",
        "y_real_test = test_data.iloc[:, 0].values\n",
        "\n",
        "# Predict on the test set\n",
        "y_real_pred = predict(X_real_test)\n",
        "\n",
        "# Accuracy\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "acc = accuracy_score(y_real_test, y_real_pred)\n",
        "print(f\"\\n Final Test Accuracy on Uploaded File: {acc * 100:.2f}%\")\n",
        "\n",
        "# Confusion Matrix\n",
        "# cm = confusion_matrix(y_real_test, y_real_pred)\n",
        "# disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "# disp.plot(cmap='Blues')\n",
        "# plt.title(\"Confusion Matrix on Real Test Set\")\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mO83dnUtterk"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMYFIQkuQtNiGnNahwHc9wH",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
